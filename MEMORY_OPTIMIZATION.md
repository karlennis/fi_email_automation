# Memory Optimization - Streaming Document Processing

## Problem
The application was crashing after processing ~220 documents with out-of-memory errors. Root cause: entire PDFs were being loaded into memory at once, and buffers weren't being properly released.

## Solution: Streaming + Zero-Copy Design

### New Implementation
Created `optimizedPdfExtractor.js` with these key improvements:

1. **One-Page-At-A-Time Processing**
   - Loads only one PDF page into memory at a time
   - Releases page immediately after extraction
   - Never holds more than ~5MB of PDF data in memory

2. **Explicit Null-ing**
   - All buffers explicitly nulled after use
   - Arrays cleared after joining
   - uint8Array explicitly filled with zeros then nulled

3. **Automatic Garbage Collection**
   - Forces GC every 5 documents processed
   - Triggered with `global.gc()` (run with `--expose-gc` flag)
   - Memory monitoring logged at each stage

4. **Memory-Aware Processing**
   - Stops extracting text once 32KB limit reached
   - Prevents accumulating excessive text
   - Respects Node.js heap limits

### Changes Made

#### 1. New File: `backend/services/optimizedPdfExtractor.js`
- `extractTextOptimized(buffer, fileName)` - PDF extraction with streaming
- `extractDocxOptimized(buffer, fileName)` - DOCX extraction with memory control
- `getMemorySummary()` - Monitor current memory usage
- `forceCleanup()` - Manual GC trigger

#### 2. Modified: `backend/services/scanJobProcessor.js`
- Added import: `const optimizedPdfExtractor = require('./optimizedPdfExtractor');`
- Replaced entire PDF/DOCX processing section (lines 570-685)
- Now uses optimized extractor for all documents

#### 3. Created: `backend/services/streamingDocumentProcessor.js`
- Advanced streaming processor (optional, for future use)
- Generator-based page streaming
- Chunk-based text extraction
- Can be used if even more optimization needed

### Performance Impact

**Before:**
- Memory: Grows linearly with document processing
- 220 documents ‚Üí Out of memory (2GB+)
- Crash after ~15 minutes

**After (Expected):**
- Memory: Stays ~200-300MB RSS regardless of documents processed
- Can process 1000+ documents without memory spike
- Stable indefinite operation

### Server Running Instructions

**IMPORTANT: Must use `--expose-gc` flag**

```bash
# In worker startup (render.yaml or your run command):
node --expose-gc --max-old-space-size=1536 worker.js

# The --expose-gc flag is CRITICAL for GC to work
# --max-old-space-size=1536 allocates 1.5GB to old space
```

### Deployment Checklist

- [ ] Commit and push changes
- [ ] Update render.yaml or deployment config to include `--expose-gc` flag
- [ ] Deploy to production
- [ ] Monitor memory usage in logs (should stay ~200-300MB)
- [ ] Run a full scan job and verify memory doesn't spike above 500MB
- [ ] Check logs for `üóëÔ∏è Forced GC` messages indicating GC is working

### Memory Monitoring

Watch for these log messages:
- `üìÑ PDF Processing Start: X - Memory: XXmb RSS` - Initial memory
- `üìÑ PDF Cleanup Complete: X - Memory: XXmb RSS` - Final memory (should be ~same)
- `üóëÔ∏è Forced GC after X documents` - GC is working
- If memory grows beyond 500MB, something else is leaking

### Fallback: If Issues Persist

If memory is still growing:
1. Increase GC interval from 5 to 3 (more aggressive)
2. Reduce `MAX_TEXT_CHARS` from 32000 to 16000 in optimizedPdfExtractor
3. Upgrade to 4GB RAM instances while investigating

### Future Optimizations

- [ ] Implement batch document processing with worker restart every 100 docs
- [ ] Use streaming for S3 downloads (currently loads entire file)
- [ ] Implement page-range extraction for very large PDFs
- [ ] Consider moving to worker pool for parallel processing
